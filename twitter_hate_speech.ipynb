{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-hate-speech.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPil6dLsOI_C"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppKiUBjOOfJ7",
        "outputId": "37c45afc-7d8d-4c93-9746-3873559a0a45"
      },
      "source": [
        "!gdown --id 1MhaG0zI4Zjt4ion5QXENwY8RK3YQfFOf"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MhaG0zI4Zjt4ion5QXENwY8RK3YQfFOf\n",
            "To: /content/Dataset for Detection of Cyber-Trolls.json.zip\n",
            "\r  0% 0.00/686k [00:00<?, ?B/s]\r100% 686k/686k [00:00<00:00, 98.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyVG6dXOix8"
      },
      "source": [
        "def fetching_information_from_resource():\n",
        "  # menentukan target file zip\n",
        "  zip_file = 'Dataset for Detection of Cyber-Trolls.json.zip'\n",
        "  extracting = zipfile.ZipFile(zip_file, 'r')\n",
        "\n",
        "  # membuat directory dataset\n",
        "  os.mkdir('dataset')\n",
        "  dataset_dir = 'dataset'\n",
        "\n",
        "  # extracting file zip\n",
        "  extracting.extractall(dataset_dir)\n",
        "  extracting.close()\n",
        "\n",
        "  # membuat variabel untuk menampung dataset yang bertipe json\n",
        "  target_file = os.path.join(dataset_dir, 'Dataset for Detection of Cyber-Trolls.json')\n",
        "  json_file = pd.read_json(target_file, lines = True)\n",
        "\n",
        "  general_datas = json_file['content']\n",
        "  general_labels = []\n",
        "\n",
        "  for index in range(len(general_datas)):\n",
        "    general_labels.append(json_file['annotation'][index]['label'][0])\n",
        "\n",
        "  indexes = list(range(len(general_labels)))\n",
        "\n",
        "  random.shuffle(indexes)\n",
        "\n",
        "  randomized_training_data = []\n",
        "  randomized_training_label = []\n",
        "\n",
        "  for index in indexes:\n",
        "    randomized_training_data.append(general_datas[index])\n",
        "    randomized_training_label.append(general_labels[index])\n",
        "\n",
        "  return (randomized_training_data, randomized_training_label)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaVL76b3QH55"
      },
      "source": [
        "def tokenizing(datas, labels):\n",
        "  VOCAB_SIZE = 1000\n",
        "  tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token = '<OOV>')\n",
        "  tokenizer.fit_on_texts(datas)\n",
        "  word_index = tokenizer.word_index\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(datas)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, padding = 'post', maxlen = 100)\n",
        "\n",
        "  integer_labels = [int(x) for x in labels]\n",
        "  arrayed_labels = np.array(integer_labels)\n",
        "\n",
        "  with open ('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return (padded_sequences, arrayed_labels)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tPZhBkPQBFA"
      },
      "source": [
        "def splitting_dataset(datas, labels, TRAINING_PORTION):\n",
        "  TRAINING_PORTION = 0.9\n",
        "\n",
        "  training_datas = datas[:round(len(datas) * TRAINING_PORTION)]\n",
        "  training_labels = labels[:round(len(datas) * TRAINING_PORTION)]\n",
        "  validation_datas = datas[round(len(datas) * TRAINING_PORTION) :]\n",
        "  validation_labels = labels[round(len(datas) * TRAINING_PORTION) :]\n",
        "\n",
        "  return (training_datas, training_labels, validation_datas, validation_labels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf7yCmhtasRr"
      },
      "source": [
        "def generate_confusion_matrix(actual_label, predicted_label):\n",
        "  matriks = tf.math.confusion_matrix(actual_label,predicted_label)\n",
        "  print(matriks)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiBKAASISYEy"
      },
      "source": [
        "def training_model(training_data, training_label, validation_data, validation_label, model_filename, model_title):\n",
        "  print(\"--- {} ---\".format(model_title))\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        1000,\n",
        "        500,\n",
        "        input_length = 100\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(16)\n",
        "    ),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(25, activation = 'relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy',tf.keras.metrics.Recall(), tf.keras.metrics.Precision()],\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  )\n",
        "\n",
        "  model.fit(\n",
        "    training_data, \n",
        "    training_label, \n",
        "    epochs = 1,\n",
        "    # validation_data = (validation_data, validation_label)\n",
        "  )\n",
        "\n",
        "  raw_predicted_label = model.predict(validation_data)\n",
        "  predicted_label = []\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
        "  for label in raw_predicted_label:\n",
        "    if label > 0.5:\n",
        "      result = 1\n",
        "    else:\n",
        "      result = 0\n",
        "    predicted_label.append(result)\n",
        "\n",
        "  generate_confusion_matrix(validation_label, predicted_label)\n",
        "\n",
        "  saved_model_path = './model/'+model_filename\n",
        "  model.save(saved_model_path)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez1v_n_qTuxd"
      },
      "source": [
        "def undersampling_dataset(training_datas, training_labels):\n",
        "  # cari kelas mayoritas\n",
        "  true_count = 0\n",
        "  false_count = 0\n",
        "\n",
        "  for index in range(len(training_datas)):\n",
        "    if training_labels[index] == 0:\n",
        "      false_count += 1\n",
        "      training_labels[index] = 0\n",
        "    elif training_labels[index] == 1:\n",
        "      true_count += 1\n",
        "      training_labels[index] = 1\n",
        "\n",
        "    if true_count > false_count:\n",
        "      majority_class = 1\n",
        "    else:\n",
        "      majority_class = 0\n",
        "\n",
        "    # masukin index data yang kelas mayoritas ke majority_indexes, sekaligus inisialisasi balanced datas dan labels\n",
        "  majority_indexes = []\n",
        "  balanced_datas = []\n",
        "  balanced_labels = []\n",
        "\n",
        "  for index in range(len(training_datas)):\n",
        "    if training_labels[index] == majority_class:\n",
        "      majority_indexes.append(index)\n",
        "    else:\n",
        "      # balanced datas dan balanced labels akan diisi sama kelas minoritas dulu\n",
        "      balanced_datas.append(training_datas[index])\n",
        "      balanced_labels.append(training_labels[index])\n",
        "\n",
        "  # randmon index dari kelas majoritas, agar yang di undersampling data acak (tidak berurutan)\n",
        "  random.shuffle(majority_indexes)\n",
        "\n",
        "  # memasukan data dari kelas mayoritas ke balanced_datas dan balanced_labels dengan banyak data setara dengan data minoritas\n",
        "  balancing_point = len(balanced_datas)\n",
        "\n",
        "  for index in range(balancing_point):\n",
        "    majority_index = majority_indexes[index]\n",
        "\n",
        "    balanced_datas.append(training_datas[majority_index])\n",
        "    balanced_labels.append(training_labels[majority_index])\n",
        "\n",
        "  # random data yang sudah balance\n",
        "  indexes = list(range(len(balanced_datas)))\n",
        "\n",
        "  random.shuffle(indexes)\n",
        "\n",
        "  randomized_training_data = []\n",
        "  randomized_training_label = []\n",
        "\n",
        "  for index in indexes:\n",
        "    randomized_training_data.append(balanced_datas[index])\n",
        "    randomized_training_label.append(balanced_labels[index])\n",
        "\n",
        "  return (np.array(randomized_training_data), np.array(randomized_training_label))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9RzWndrPz8v",
        "outputId": "b2c2355e-3bd7-47b8-fc07-b19a77f7887f"
      },
      "source": [
        "# (general_datas, general_labels) = fetching_information_from_resource()\n",
        "\n",
        "(tokenized_datas, arrayed_labels) = tokenizing(general_datas, general_labels)\n",
        "\n",
        "(\n",
        "    training_datas, \n",
        "    training_labels, \n",
        "    validation_datas, \n",
        "    validation_labels\n",
        ") = splitting_dataset(tokenized_datas, arrayed_labels, 0.9)\n",
        "unbalanced_model = training_model(training_datas, training_labels, validation_datas, validation_labels, 'unbalanced_model.h5', 'UNBALANCED MODEL')\n",
        "\n",
        "(balanced_datas, balanced_labels) = undersampling_dataset(tokenized_datas, arrayed_labels)\n",
        "\n",
        "balanced_model = training_model(balanced_datas, balanced_labels, validation_datas, validation_labels, 'balanced_model.h5', 'BALANCED MODEL')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UNBALANCED MODEL ---\n",
            "563/563 [==============================] - 96s 87ms/step - loss: 0.5431 - accuracy: 0.6861 - recall_3: 0.5499 - precision_3: 0.6099\n",
            "tf.Tensor(\n",
            "[[906 319]\n",
            " [191 584]], shape=(2, 2), dtype=int32)\n",
            "--- BALANCED MODEL ---\n",
            "489/489 [==============================] - 45s 87ms/step - loss: 0.5596 - accuracy: 0.6943 - recall_4: 0.8266 - precision_4: 0.6537\n",
            "tf.Tensor(\n",
            "[[720 505]\n",
            " [ 72 703]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}