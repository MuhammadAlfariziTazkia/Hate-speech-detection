{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter-hate-speech.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPil6dLsOI_C"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppKiUBjOOfJ7",
        "outputId": "244813f9-0296-46a0-8255-d2afe55d1c19"
      },
      "source": [
        "!gdown --id 1MhaG0zI4Zjt4ion5QXENwY8RK3YQfFOf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1MhaG0zI4Zjt4ion5QXENwY8RK3YQfFOf\n",
            "To: /content/Dataset for Detection of Cyber-Trolls.json.zip\n",
            "\r  0% 0.00/686k [00:00<?, ?B/s]\r100% 686k/686k [00:00<00:00, 45.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pyVG6dXOix8"
      },
      "source": [
        "def fetching_information_from_resource():\n",
        "  # menentukan target file zip\n",
        "  zip_file = 'Dataset for Detection of Cyber-Trolls.json.zip'\n",
        "  extracting = zipfile.ZipFile(zip_file, 'r')\n",
        "\n",
        "  # membuat directory dataset\n",
        "  os.mkdir('dataset')\n",
        "  dataset_dir = 'dataset'\n",
        "\n",
        "  # extracting file zip\n",
        "  extracting.extractall(dataset_dir)\n",
        "  extracting.close()\n",
        "\n",
        "  # membuat variabel untuk menampung dataset yang bertipe json\n",
        "  target_file = os.path.join(dataset_dir, 'Dataset for Detection of Cyber-Trolls.json')\n",
        "  json_file = pd.read_json(target_file, lines = True)\n",
        "\n",
        "  general_datas = json_file['content']\n",
        "  general_labels = []\n",
        "\n",
        "  for index in range(len(general_datas)):\n",
        "    general_labels.append(json_file['annotation'][index]['label'][0])\n",
        "\n",
        "  indexes = list(range(len(general_labels)))\n",
        "\n",
        "  random.shuffle(indexes)\n",
        "\n",
        "  randomized_training_data = []\n",
        "  randomized_training_label = []\n",
        "\n",
        "  for index in indexes:\n",
        "    randomized_training_data.append(general_datas[index])\n",
        "    randomized_training_label.append(general_labels[index])\n",
        "\n",
        "  return (randomized_training_data, randomized_training_label)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaVL76b3QH55"
      },
      "source": [
        "def tokenizing(datas, labels):\n",
        "  VOCAB_SIZE = 1000\n",
        "  tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token = '<OOV>')\n",
        "  tokenizer.fit_on_texts(datas)\n",
        "  word_index = tokenizer.word_index\n",
        "\n",
        "  sequences = tokenizer.texts_to_sequences(datas)\n",
        "\n",
        "  padded_sequences = pad_sequences(sequences, padding = 'post', maxlen = 100)\n",
        "\n",
        "  integer_labels = [int(x) for x in labels]\n",
        "  arrayed_labels = np.array(integer_labels)\n",
        "\n",
        "  with open ('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return (padded_sequences, arrayed_labels)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tPZhBkPQBFA"
      },
      "source": [
        "def splitting_dataset(datas, labels, TRAINING_PORTION):\n",
        "  TRAINING_PORTION = 0.9\n",
        "\n",
        "  training_datas = datas[:round(len(datas) * TRAINING_PORTION)]\n",
        "  training_labels = labels[:round(len(datas) * TRAINING_PORTION)]\n",
        "  validation_datas = datas[round(len(datas) * TRAINING_PORTION) :]\n",
        "  validation_labels = labels[round(len(datas) * TRAINING_PORTION) :]\n",
        "\n",
        "  return (training_datas, training_labels, validation_datas, validation_labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf7yCmhtasRr"
      },
      "source": [
        "def generate_confusion_matrix(actual_label, predicted_label):\n",
        "  matriks = tf.math.confusion_matrix(actual_label,predicted_label)\n",
        "  print(matriks)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiBKAASISYEy"
      },
      "source": [
        "def training_model(training_data, training_label, validation_data, validation_label, model_filename, model_title):\n",
        "  print(\"--- {} ---\".format(model_title))\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        1000,\n",
        "        500,\n",
        "        input_length = 100\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "      tf.keras.layers.LSTM(16)\n",
        "    ),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(25, activation = 'relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(50, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "  ])\n",
        "\n",
        "  model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy',tf.keras.metrics.Recall(), tf.keras.metrics.Precision()],\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "  )\n",
        "\n",
        "  model.fit(\n",
        "    training_data, \n",
        "    training_label, \n",
        "    epochs = 10,\n",
        "    validation_data = (validation_data, validation_label)\n",
        "  )\n",
        "\n",
        "  raw_predicted_label = model.predict(validation_data)\n",
        "  predicted_label = []\n",
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
        "  for label in raw_predicted_label:\n",
        "    if label > 0.5:\n",
        "      result = 1\n",
        "    else:\n",
        "      result = 0\n",
        "    predicted_label.append(result)\n",
        "\n",
        "  generate_confusion_matrix(validation_label, predicted_label)\n",
        "\n",
        "  saved_model_path = './model/'+model_filename\n",
        "  model.save(saved_model_path)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez1v_n_qTuxd"
      },
      "source": [
        "def undersampling_dataset(training_datas, training_labels):\n",
        "  # cari kelas mayoritas\n",
        "  true_count = 0\n",
        "  false_count = 0\n",
        "\n",
        "  for index in range(len(training_datas)):\n",
        "    if training_labels[index] == 0:\n",
        "      false_count += 1\n",
        "      training_labels[index] = 0\n",
        "    elif training_labels[index] == 1:\n",
        "      true_count += 1\n",
        "      training_labels[index] = 1\n",
        "\n",
        "    if true_count > false_count:\n",
        "      majority_class = 1\n",
        "    else:\n",
        "      majority_class = 0\n",
        "\n",
        "    # masukin index data yang kelas mayoritas ke majority_indexes, sekaligus inisialisasi balanced datas dan labels\n",
        "  majority_indexes = []\n",
        "  balanced_datas = []\n",
        "  balanced_labels = []\n",
        "\n",
        "  for index in range(len(training_datas)):\n",
        "    if training_labels[index] == majority_class:\n",
        "      majority_indexes.append(index)\n",
        "    else:\n",
        "      # balanced datas dan balanced labels akan diisi sama kelas minoritas dulu\n",
        "      balanced_datas.append(training_datas[index])\n",
        "      balanced_labels.append(training_labels[index])\n",
        "\n",
        "  # randmon index dari kelas majoritas, agar yang di undersampling data acak (tidak berurutan)\n",
        "  random.shuffle(majority_indexes)\n",
        "\n",
        "  # memasukan data dari kelas mayoritas ke balanced_datas dan balanced_labels dengan banyak data setara dengan data minoritas\n",
        "  balancing_point = len(balanced_datas)\n",
        "\n",
        "  for index in range(balancing_point):\n",
        "    majority_index = majority_indexes[index]\n",
        "\n",
        "    balanced_datas.append(training_datas[majority_index])\n",
        "    balanced_labels.append(training_labels[majority_index])\n",
        "\n",
        "  # random data yang sudah balance\n",
        "  indexes = list(range(len(balanced_datas)))\n",
        "\n",
        "  random.shuffle(indexes)\n",
        "\n",
        "  randomized_training_data = []\n",
        "  randomized_training_label = []\n",
        "\n",
        "  for index in indexes:\n",
        "    randomized_training_data.append(balanced_datas[index])\n",
        "    randomized_training_label.append(balanced_labels[index])\n",
        "\n",
        "  return (np.array(randomized_training_data), np.array(randomized_training_label))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9RzWndrPz8v",
        "outputId": "43fc01ab-900f-4764-95d5-15843c73216e"
      },
      "source": [
        "# (general_datas, general_labels) = fetching_information_from_resource()\n",
        "\n",
        "(tokenized_datas, arrayed_labels) = tokenizing(general_datas, general_labels)\n",
        "\n",
        "(\n",
        "    training_datas, \n",
        "    training_labels, \n",
        "    validation_datas, \n",
        "    validation_labels\n",
        ") = splitting_dataset(tokenized_datas, arrayed_labels, 0.9)\n",
        "unbalanced_model = training_model(training_datas, training_labels, validation_datas, validation_labels, 'unbalanced_model.h5', 'UNBALANCED MODEL')\n",
        "print()\n",
        "print()\n",
        "(balanced_datas, balanced_labels) = undersampling_dataset(tokenized_datas, arrayed_labels)\n",
        "\n",
        "balanced_model = training_model(balanced_datas, balanced_labels, validation_datas, validation_labels, 'balanced_model.h5', 'BALANCED MODEL')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- UNBALANCED MODEL ---\n",
            "Epoch 1/10\n",
            "563/563 [==============================] - 79s 134ms/step - loss: 0.5399 - accuracy: 0.6949 - recall_2: 0.5754 - precision_2: 0.6193 - val_loss: 0.4717 - val_accuracy: 0.7440 - val_recall_2: 0.6462 - val_precision_2: 0.6726\n",
            "Epoch 2/10\n",
            "563/563 [==============================] - 75s 134ms/step - loss: 0.4469 - accuracy: 0.7749 - recall_2: 0.7260 - precision_2: 0.7074 - val_loss: 0.4508 - val_accuracy: 0.7675 - val_recall_2: 0.7154 - val_precision_2: 0.6893\n",
            "Epoch 3/10\n",
            "563/563 [==============================] - 76s 134ms/step - loss: 0.3913 - accuracy: 0.8189 - recall_2: 0.7871 - precision_2: 0.7596 - val_loss: 0.4243 - val_accuracy: 0.7820 - val_recall_2: 0.7010 - val_precision_2: 0.7218\n",
            "Epoch 4/10\n",
            "563/563 [==============================] - 74s 132ms/step - loss: 0.3403 - accuracy: 0.8512 - recall_2: 0.8349 - precision_2: 0.7955 - val_loss: 0.4149 - val_accuracy: 0.8130 - val_recall_2: 0.8068 - val_precision_2: 0.7322\n",
            "Epoch 5/10\n",
            "563/563 [==============================] - 76s 134ms/step - loss: 0.2834 - accuracy: 0.8790 - recall_2: 0.8756 - precision_2: 0.8262 - val_loss: 0.4042 - val_accuracy: 0.8265 - val_recall_2: 0.8629 - val_precision_2: 0.7320\n",
            "Epoch 6/10\n",
            "563/563 [==============================] - 76s 134ms/step - loss: 0.2399 - accuracy: 0.9046 - recall_2: 0.9138 - precision_2: 0.8532 - val_loss: 0.3871 - val_accuracy: 0.8490 - val_recall_2: 0.8407 - val_precision_2: 0.7816\n",
            "Epoch 7/10\n",
            "563/563 [==============================] - 76s 135ms/step - loss: 0.1939 - accuracy: 0.9252 - recall_2: 0.9338 - precision_2: 0.8822 - val_loss: 0.4048 - val_accuracy: 0.8580 - val_recall_2: 0.9164 - val_precision_2: 0.7614\n",
            "Epoch 8/10\n",
            "563/563 [==============================] - 75s 134ms/step - loss: 0.1684 - accuracy: 0.9382 - recall_2: 0.9443 - precision_2: 0.9026 - val_loss: 0.4320 - val_accuracy: 0.8570 - val_recall_2: 0.9125 - val_precision_2: 0.7614\n",
            "Epoch 9/10\n",
            "563/563 [==============================] - 74s 132ms/step - loss: 0.1413 - accuracy: 0.9481 - recall_2: 0.9544 - precision_2: 0.9166 - val_loss: 0.4472 - val_accuracy: 0.8765 - val_recall_2: 0.9217 - val_precision_2: 0.7906\n",
            "Epoch 10/10\n",
            "563/563 [==============================] - 75s 134ms/step - loss: 0.1183 - accuracy: 0.9571 - recall_2: 0.9619 - precision_2: 0.9309 - val_loss: 0.4742 - val_accuracy: 0.8785 - val_recall_2: 0.9360 - val_precision_2: 0.7870\n",
            "tf.Tensor(\n",
            "[[1040  194]\n",
            " [  49  717]], shape=(2, 2), dtype=int32)\n",
            "\n",
            "\n",
            "--- BALANCED MODEL ---\n",
            "Epoch 1/10\n",
            "489/489 [==============================] - 72s 138ms/step - loss: 0.5586 - accuracy: 0.6879 - recall_3: 0.7969 - precision_3: 0.6542 - val_loss: 0.4605 - val_accuracy: 0.7425 - val_recall_3: 0.8956 - val_precision_3: 0.6120\n",
            "Epoch 2/10\n",
            "489/489 [==============================] - 67s 138ms/step - loss: 0.4495 - accuracy: 0.7822 - recall_3: 0.8517 - precision_3: 0.7477 - val_loss: 0.3946 - val_accuracy: 0.8110 - val_recall_3: 0.8851 - val_precision_3: 0.7004\n",
            "Epoch 3/10\n",
            "489/489 [==============================] - 67s 138ms/step - loss: 0.3894 - accuracy: 0.8215 - recall_3: 0.8664 - precision_3: 0.7950 - val_loss: 0.3801 - val_accuracy: 0.8220 - val_recall_3: 0.9413 - val_precision_3: 0.6986\n",
            "Epoch 4/10\n",
            "489/489 [==============================] - 68s 138ms/step - loss: 0.3344 - accuracy: 0.8556 - recall_3: 0.8916 - precision_3: 0.8317 - val_loss: 0.3105 - val_accuracy: 0.8705 - val_recall_3: 0.9034 - val_precision_3: 0.7891\n",
            "Epoch 5/10\n",
            "489/489 [==============================] - 68s 138ms/step - loss: 0.2809 - accuracy: 0.8863 - recall_3: 0.9119 - precision_3: 0.8676 - val_loss: 0.2753 - val_accuracy: 0.8890 - val_recall_3: 0.9399 - val_precision_3: 0.8036\n",
            "Epoch 6/10\n",
            "489/489 [==============================] - 68s 138ms/step - loss: 0.2328 - accuracy: 0.9119 - recall_3: 0.9285 - precision_3: 0.8986 - val_loss: 0.2809 - val_accuracy: 0.8955 - val_recall_3: 0.9595 - val_precision_3: 0.8050\n",
            "Epoch 7/10\n",
            "489/489 [==============================] - 68s 139ms/step - loss: 0.1946 - accuracy: 0.9278 - recall_3: 0.9407 - precision_3: 0.9171 - val_loss: 0.2559 - val_accuracy: 0.9115 - val_recall_3: 0.9726 - val_precision_3: 0.8269\n",
            "Epoch 8/10\n",
            "489/489 [==============================] - 68s 139ms/step - loss: 0.1552 - accuracy: 0.9453 - recall_3: 0.9587 - precision_3: 0.9338 - val_loss: 0.2669 - val_accuracy: 0.9150 - val_recall_3: 0.9896 - val_precision_3: 0.8239\n",
            "Epoch 9/10\n",
            "489/489 [==============================] - 67s 137ms/step - loss: 0.1332 - accuracy: 0.9521 - recall_3: 0.9631 - precision_3: 0.9423 - val_loss: 0.2606 - val_accuracy: 0.9200 - val_recall_3: 0.9817 - val_precision_3: 0.8374\n",
            "Epoch 10/10\n",
            "489/489 [==============================] - 68s 139ms/step - loss: 0.1114 - accuracy: 0.9613 - recall_3: 0.9724 - precision_3: 0.9512 - val_loss: 0.2332 - val_accuracy: 0.9375 - val_recall_3: 0.9856 - val_precision_3: 0.8688\n",
            "tf.Tensor(\n",
            "[[1120  114]\n",
            " [  11  755]], shape=(2, 2), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGhMVYQPgQDO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}